name = "FlashAttentionWrapper"
uuid = "4822a80a-c2d5-4bbd-b1da-f89b711347c8"
authors = ["Jun Tian <tianjun.cpp@gmail.com>"]
version = "0.1.0"

[deps]
ChainRulesCore = "d360d2e6-b24c-11e9-a2a3-2a2ae2dbcce4"
CondaPkg = "992eb4ea-22a4-4c89-a5bb-47a3300528ab"
DLPack = "53c2dc0f-f7d5-43fd-8906-6c0220547083"
LuxCore = "bb33d45b-7691-41d6-9220-0943567d0623"
PythonCall = "6099a3de-0909-46bc-b1f4-468b9a2dfc0d"
Random = "9a3f8284-a2c9-5f02-9a11-845980a1fd5c"

[compat]
ChainRulesCore = "1"
CondaPkg = "0.2"
DLPack = "0.3"
LuxCore = "1"
PythonCall = "0.9"
Random = "1"
julia = "1.8"

[extras]
CUDA = "052768ef-5323-5732-b1bb-66c8b64840ba"
NNlib = "872c559c-99b0-510c-b3b7-b6c96a88d5cd"
Test = "8dfed614-e22c-5e08-85e1-65c5234f0b40"
Zygote = "e88e6eb3-aa80-5325-afca-941959d7151f"

[targets]
test = ["Test", "CUDA", "NNlib", "Zygote"]